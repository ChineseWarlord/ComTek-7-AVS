{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from src.dataset import FrameDataset, custom_collate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torch\n",
    "from src.averager import Averager\n",
    "import os\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "from src import config\n",
    "hparams = config.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(['apadding', 'bicycle','human', 'motorcycle', 'vehicle'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = FrameDataset(root = hparams.train_path, label_encoder = le)\n",
    "train_dataloader = DataLoader(training_data, batch_size=hparams.batch_size, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model; pre-trained on COCO\n",
    "model = fasterrcnn_resnet50_fpn(num_classes=hparams.num_classes)\n",
    "device = torch.device(hparams.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #50 loss: 0.36814434403107865\n",
      "Iteration #100 loss: 0.3358881357702124\n",
      "Iteration #150 loss: 0.343179546844417\n",
      "Iteration #200 loss: 0.5043411239549764\n",
      "Iteration #250 loss: 0.4155584117938979\n",
      "Iteration #300 loss: 0.27577452137573527\n",
      "Iteration #350 loss: 0.24899384515620787\n",
      "Iteration #400 loss: 0.5403421564894159\n",
      "Iteration #450 loss: 0.5971994017380827\n",
      "Iteration #500 loss: 0.30830810682326004\n",
      "Iteration #550 loss: 0.3175986090868016\n",
      "Iteration #600 loss: 0.45723234071506036\n",
      "Iteration #650 loss: 0.675758758796615\n",
      "Iteration #700 loss: 0.6444362851685171\n",
      "Iteration #750 loss: 1.007549514312901\n",
      "Iteration #800 loss: 0.8187508652914958\n",
      "Iteration #850 loss: 0.17557743551587912\n",
      "Iteration #900 loss: 0.38316122597116803\n",
      "Epoch #0 loss: 0.5305003648014625\n",
      "Iteration #950 loss: 0.4251491779980801\n",
      "Iteration #1000 loss: 0.6151012527848833\n",
      "Iteration #1050 loss: 0.1465978109034619\n",
      "Iteration #1100 loss: 0.5024560375317986\n",
      "Iteration #1150 loss: 0.563880212736261\n",
      "Iteration #1200 loss: 0.3076834321546425\n",
      "Iteration #1250 loss: 0.09080299325274346\n",
      "Iteration #1300 loss: 0.6710548098260669\n",
      "Iteration #1350 loss: 0.5744176929182957\n",
      "Iteration #1400 loss: 0.43797301327602045\n",
      "Iteration #1450 loss: 0.5339328961416876\n",
      "Iteration #1500 loss: 0.3417439240081654\n",
      "Iteration #1550 loss: 0.6257610612102347\n",
      "Iteration #1600 loss: 0.7536388279283781\n",
      "Iteration #1650 loss: 0.4879289751416402\n",
      "Iteration #1700 loss: 0.46587875412713925\n",
      "Iteration #1750 loss: 0.5981849007177822\n",
      "Iteration #1800 loss: 0.6376162265496941\n",
      "Iteration #1850 loss: 0.5576059417940813\n",
      "Epoch #1 loss: 0.46146729664186525\n",
      "Iteration #1900 loss: 0.5215164355373904\n",
      "Iteration #1950 loss: 0.5952065332591074\n",
      "Iteration #2000 loss: 0.25872235678743816\n",
      "Iteration #2050 loss: 0.5254880299123467\n",
      "Iteration #2100 loss: 0.3259671726994925\n",
      "Iteration #2150 loss: 0.31296648806865857\n",
      "Iteration #2200 loss: 0.501695549929363\n",
      "Iteration #2250 loss: 0.7315560765229273\n",
      "Iteration #2300 loss: 0.7652701580826834\n",
      "Iteration #2350 loss: 0.10994463784616187\n",
      "Iteration #2400 loss: 0.22038504583487598\n",
      "Iteration #2450 loss: 0.5032728819374863\n",
      "Iteration #2500 loss: 0.1455465278934842\n",
      "Iteration #2550 loss: 0.5208082615322005\n",
      "Iteration #2600 loss: 0.5486030884744872\n",
      "Iteration #2650 loss: 0.4114688227097183\n",
      "Iteration #2700 loss: 0.3988838560354142\n",
      "Iteration #2750 loss: 0.38393102541511126\n",
      "Iteration #2800 loss: 0.20203835825946695\n",
      "Epoch #2 loss: 0.4229501139722273\n",
      "Iteration #2850 loss: 0.14028003168447115\n",
      "Iteration #2900 loss: 0.39488593528040883\n",
      "Iteration #2950 loss: 0.49840311683562966\n",
      "Iteration #3000 loss: 0.6189506400301399\n",
      "Iteration #3050 loss: 0.48240315643987913\n",
      "Iteration #3100 loss: 0.48134079314363853\n",
      "Iteration #3150 loss: 0.2252442455910038\n",
      "Iteration #3200 loss: 0.20585530087361734\n",
      "Iteration #3250 loss: 0.5510064928358985\n",
      "Iteration #3300 loss: 0.594173327841192\n",
      "Iteration #3350 loss: 0.307211398484419\n",
      "Iteration #3400 loss: 0.5085876832188891\n",
      "Iteration #3450 loss: 0.305028744411912\n",
      "Iteration #3500 loss: 0.48173129288096284\n",
      "Iteration #3550 loss: 0.27626070642130307\n",
      "Iteration #3600 loss: 0.47215570877833163\n",
      "Iteration #3650 loss: 0.47921625327257733\n",
      "Iteration #3700 loss: 0.3825542627052903\n",
      "Iteration #3750 loss: 0.2562011939947545\n",
      "Epoch #3 loss: 0.39636247339649133\n",
      "Iteration #3800 loss: 0.3367497205464588\n",
      "Iteration #3850 loss: 0.3882014172319133\n",
      "Iteration #3900 loss: 0.4141426072054432\n",
      "Iteration #3950 loss: 0.42316258666720263\n",
      "Iteration #4000 loss: 0.37643397058385564\n",
      "Iteration #4050 loss: 0.37666063468398975\n",
      "Iteration #4100 loss: 0.20520116159282958\n",
      "Iteration #4150 loss: 0.8333624119412958\n",
      "Iteration #4200 loss: 0.3906070086309513\n",
      "Iteration #4250 loss: 0.8190018509631755\n",
      "Iteration #4300 loss: 0.3849840487344979\n",
      "Iteration #4350 loss: 0.3504332339155138\n",
      "Iteration #4400 loss: 0.3252314721042158\n",
      "Iteration #4450 loss: 0.24489385630891236\n",
      "Iteration #4500 loss: 0.5277944022795681\n",
      "Iteration #4550 loss: 0.36290815646577473\n",
      "Iteration #4600 loss: 0.31103242819741267\n",
      "Iteration #4650 loss: 0.20453246867798042\n",
      "Iteration #4700 loss: 0.29612625837622575\n",
      "Epoch #4 loss: 0.3768308114382971\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=hparams.learning_rate, momentum=0.9, weight_decay=0.00001)\n",
    "\n",
    "\n",
    "loss_hist = Averager()\n",
    "itr = 1\n",
    "\n",
    "for epoch in range(hparams.max_epochs):\n",
    "    loss_hist.reset()\n",
    "    \n",
    "    for frames, targets in train_dataloader:\n",
    "        \n",
    "        frames = [frame.to(device) for frame in frames]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(frames, targets)   ##Return the loss\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)  #Average out the loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 200 == 0:\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "\n",
    "    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the output directory exists\n",
    "log_name = log_name = f\"{hparams.experiment_name}-{hparams.num_classes}\"\n",
    "output_dir = os.path.join(hparams.output_dir,log_name)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "path = os.path.join(output_dir,  log_name + str(epoch) + '.pth')\n",
    "torch.save(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('products/object_detection-5/object_detection-54.pth')\n",
    "model.eval()\n",
    "model.to(torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "video_path = 'data/test/clips/20200515/clip_28_1250.mp4'\n",
    "cap = cv.VideoCapture(video_path)\n",
    "\n",
    "counter = 0\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    if (ret == True) & (counter < 5):\n",
    "        tens_frame = (frame/255).transpose((2, 0, 1))\n",
    "        tens_frame = torch.tensor(tens_frame).type(torch.float).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            prediction = model(tens_frame)\n",
    "\n",
    "        boxes = prediction[0]['boxes'].numpy()\n",
    "        labels = prediction[0]['labels'].numpy()\n",
    "        scores = prediction[0]['scores'].numpy()\n",
    "        labels = le.inverse_transform(labels)\n",
    "\n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            if score > 0.8:\n",
    "                x1,y1,x2,y2 = int(box[0]), int(box[1]), int(box[2]), int(box[3])\n",
    "\n",
    "                cv.putText(frame,label,(x1,y1-4),cv.FONT_HERSHEY_SIMPLEX,0.4,(0,255,0),1,cv.LINE_AA)\n",
    "                cv.rectangle(frame,(x1, y1),(x2, y2),(255,0,0),1)\n",
    "        \n",
    "        cv.imshow('video', frame)\n",
    "        counter +=1\n",
    "        if cv.waitKey(25) & 0xFF == ord('q'):\n",
    "            cv.destroyAllWindows()\n",
    "            break\n",
    "        \n",
    "        \n",
    " \n",
    "  # Break the loop\n",
    "    else: \n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 384)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
