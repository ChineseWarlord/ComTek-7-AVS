{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise lecture 8 - Multilayer Perceptron\n",
    "Develop an MLP for the MNIST\n",
    "database by using the dimension-reduced data from your work on Exercises 2 and 3. You can download the LDA projected data here.\n",
    "Further, you can use 10-, 20- and 30-dimensional data generated by PCA and compare\n",
    "their performance (at the same time, try various MLP architectures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: \n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "train_targets: \n",
      "[0 0 0 ... 9 9 9], shape: (60000,)\n",
      "train_set: \n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape: (60000, 784)\n",
      "test_targets: \n",
      "[0 0 0 ... 9 9 9], shape: (10000,)\n",
      "test_set: \n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.neural_network import MLPClassifier as MLPC\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore') #Ignore warnings\n",
    "\n",
    "def create_complete_datasets(data_dict):\n",
    "    '''\n",
    "    Function for creating complete training and test sets containing\n",
    "    all classes.\n",
    "    '''\n",
    "    #Empty list\n",
    "    trainset = []\n",
    "    traintargets =[]\n",
    "    testset = []\n",
    "    testtargets =[]\n",
    "    \n",
    "    #For each class\n",
    "    for i in range(10):\n",
    "        trainset.append(data_dict[\"train%d\"%i])\n",
    "        traintargets.append(np.full(len(data_dict[\"train%d\"%i]),i))\n",
    "        testset.append(data_dict[\"test%d\"%i])\n",
    "        testtargets.append(np.full(len(data_dict[\"test%d\"%i]),i))\n",
    "    \n",
    "    #Concatenate into to complete datasets\n",
    "    trainset = np.concatenate(trainset)\n",
    "    traintargets = np.concatenate(traintargets)\n",
    "    testset = np.concatenate(testset)\n",
    "    testtargets = np.concatenate(testtargets)\n",
    "    return trainset, traintargets, testset, testtargets\n",
    "\n",
    "file = \"Data/mnist_all.mat\"\n",
    "data = loadmat(file)\n",
    "\n",
    "#Complete training and test sets\n",
    "train_set, train_targets, test_set, test_targets = create_complete_datasets(data)\n",
    "train_set = train_set/255\n",
    "test_set = test_set/255\n",
    "classes = np.arange(10)\n",
    "\n",
    "print(f\"Classes: \\n{classes}\")\n",
    "print(f\"train_targets: \\n{train_targets}, shape: {train_targets.shape}\")\n",
    "print(f\"train_set: \\n{train_set}, shape: {train_set.shape}\")\n",
    "print(f\"test_targets: \\n{test_targets}, shape: {test_targets.shape}\")\n",
    "print(f\"test_set: \\n{test_set}, shape: {test_set.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating PCA/LDA data\n",
    "From previous exercises we already know how to use PCA/LDA.\n",
    "We first fit a PCA/LDA model to our training data, and then transform the training and test data using this model, to get a dimensionality reduced data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PCA.fit_transform() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mg:\\Nextcloud\\COMTEK_7_GITHUB\\ComTek-7-AVS\\Lecture 8\\exercise.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Nextcloud/COMTEK_7_GITHUB/ComTek-7-AVS/Lecture%208/exercise.ipynb#X64sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# PCA\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Nextcloud/COMTEK_7_GITHUB/ComTek-7-AVS/Lecture%208/exercise.ipynb#X64sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39mn_components)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/Nextcloud/COMTEK_7_GITHUB/ComTek-7-AVS/Lecture%208/exercise.ipynb#X64sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pca\u001b[39m.\u001b[39;49mfit_transform()(train_set)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Nextcloud/COMTEK_7_GITHUB/ComTek-7-AVS/Lecture%208/exercise.ipynb#X64sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPCA shape: \u001b[39m\u001b[39m{\u001b[39;00mpca\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Nextcloud/COMTEK_7_GITHUB/ComTek-7-AVS/Lecture%208/exercise.ipynb#X64sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# LDA\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: PCA.fit_transform() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "n_components = 9\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit_transform()(train_set)\n",
    "print(f\"PCA shape: {pca.shape}\")\n",
    "\n",
    "# LDA\n",
    "lda = LDA(n_components=n_components)\n",
    "lda.fit_transform()(train_set, train_targets)\n",
    "print(f\"LDA shape: {lda.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training MLP\n",
    "Sklearn has a multilayer perceptron classifier which we can use:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html \n",
    "\n",
    "To use it, we need to choose how many layers we would like to use and the size of each hidden layer. \n",
    "We can also choose which non-linear activation function to use and what optimizer/solver to use.\n",
    "You can set a maximum iteration number as well (to limit compute time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model trained on LDA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate MLP with your parameters of choice and train it on the LDA dimensionality reduced training data.\n",
    "# Afterwards test it on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model trained on PCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate MLP with your parameters of choice and train it on the PCA dimensionality reduced training data.\n",
    "# Afterwards test it on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the trained models\n",
    "With models trained on both LDA and PCA data, let's compare them using the confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%Confusion matrix\n",
    "#Compute the confusion matrices for both MLPs (trained on PCA and LDA) and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can we conclude from the comparison?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
